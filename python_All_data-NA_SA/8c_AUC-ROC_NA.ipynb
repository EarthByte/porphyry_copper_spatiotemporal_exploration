{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the parameters in use. Check the parameters carefully to ensure a sucussful run.\n",
      "{ 'agegrid_url': 'https://www.earthbyte.org/webdav/ftp/Data_Collections/Muller_etal_2016_AREPS/Muller_etal_2016_AREPS_Agegrids/Muller_etal_2016_AREPS_Agegrids_v1.17/Muller_etal_2016_AREPS_v1.17_netCDF/Muller_etal_2016_AREPS_v1.17_AgeGrid-{}.nc',\n",
      "  'anchor_plate_id': 0,\n",
      "  'case_name': 'case_AREPS',\n",
      "  'coastlines': '../data/Global_EarthByte_230-0Ma_GK07_AREPS_Coastlines.gpmlz',\n",
      "  'convergence_data_dir': './convergence_data/AREPS/',\n",
      "  'convergence_data_filename_ext': 'csv',\n",
      "  'convergence_data_filename_prefix': 'subStats',\n",
      "  'coreg_input_dir': './coreg_input/AREPS/',\n",
      "  'coreg_input_files': [ 'deposit_candidates.csv',\n",
      "                         'negative_deposits.csv',\n",
      "                         'positive_deposits.csv'],\n",
      "  'coreg_output_dir': 'coreg_output',\n",
      "  'feature_names': [ 'conv_rate',\n",
      "                     'dist_nearest_edge',\n",
      "                     'subduction_volume_km3y',\n",
      "                     'carbonate_sediment_thickness',\n",
      "                     'ocean_crust_carb_percent'],\n",
      "  'grid_files': [ [ '../data/AgeGrids/Muller_etal_2016_AREPS_v1.17_AgeGrid-{time:d}.nc',\n",
      "                    'seafloor_age'],\n",
      "                  [ '../data/carbonate_sed_thickness/decompacted_sediment_thickness_0.5_{time:d}.nc',\n",
      "                    'carbonate_sediment_thickness'],\n",
      "                  [ '../data/predicted_oceanic_sediment_thickness/sed_thick_0.2d_{time:d}.nc',\n",
      "                    'total_sediment_thick'],\n",
      "                  [ '../data/ocean_crust_CO2_grids/ocean_crust_carb_percent_{time:d}.nc',\n",
      "                    'ocean_crust_carb_percent']],\n",
      "  'machine_learning_engine': 'RFC',\n",
      "  'map_extent': [-85, -29, -55, 15],\n",
      "  'ml_input_dir': 'ml_input',\n",
      "  'ml_output_dir': 'ml_output',\n",
      "  'overwrite_existing_convergence_data': True,\n",
      "  'plate_tectonic_tools_path': '/workspace/spatio-temporal-exploration-master-JDiazRod/python/../PlateTectonicTools/ptt/',\n",
      "  'region_of_interest_polygon_file': '../data/polygon_north_america_julian.csv',\n",
      "  'regions': [5, 10, 30],\n",
      "  'rotation_files': ['../data/Global_EarthByte_230-0Ma_GK07_AREPS.rot'],\n",
      "  'static_polygons_file': '../data/Shapefiles/StaticPolygons/Global_EarthByte_GPlates_PresentDay_StaticPlatePolygons_2015_v1.shp',\n",
      "  'terranes': '',\n",
      "  'threshold_sampling_distance_degrees': 0.2,\n",
      "  'time': {'end': 230, 'start': 0, 'step': 1},\n",
      "  'topo_grid': '../data/topo15_3600x1800.nc',\n",
      "  'topology_files': [ '../data/Global_EarthByte_230-0Ma_GK07_AREPS_PlateBoundaries.gpml.gz',\n",
      "                      '../data/Global_EarthByte_230-0Ma_GK07_AREPS_Topology_BuildingBlocks.gpml.gz'],\n",
      "  'vector_files': ['{conv_dir}subStats_{time:.2f}.csv'],\n",
      "  'velocity_delta_time': 1}\n"
     ]
    }
   ],
   "source": [
    "from parameters import parameters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Utils\n",
    "#\n",
    "# LOOK HERE! \n",
    "# This cell must run first to setup the working environment\n",
    "#\n",
    "\n",
    "#load the config file\n",
    "Utils.load_config('config.json')\n",
    "Utils.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "from parameters import parameters \n",
    "import Utils as Utils\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "\n",
    "names = [\"Linear SVM\",\n",
    "         \"SVC-POLY\",\n",
    "         #\"SVC-POLY-C=100\",\n",
    "         \"SVC-RBF\",\n",
    "         #\"SVC-RBF-C=100\",\n",
    "         \"SVC-SIGM\",\n",
    "         #\"SVC-SIGM-C=100\",\n",
    "        #\"GaussianProcess-1\",\n",
    "         \"GaussianProcess\",\n",
    "         \"Decision Tree\",\n",
    "         \"RandomForest\",\n",
    "         #\"NN-1e-05\",\n",
    "         \"Neural_Network\",\n",
    "         \"Naive Bayes\",\n",
    "         \"QDA\",\n",
    "         \"AdaBoost\"\n",
    "         #\"AdaBoost-100\"\n",
    "]\n",
    "\n",
    "classifiers = [SVC(kernel=\"linear\", C=0.025, probability=True),\n",
    "    SVC(kernel='poly', degree=3, gamma=2, C=10, probability=True),\n",
    "    #SVC(kernel='poly', degree=3, gamma=2, C=100, probability=True),\n",
    "    SVC(kernel='rbf', gamma=2, C=10, probability=True),\n",
    "    #SVC(kernel='rbf', gamma=2, C=100, probability=True),\n",
    "    SVC(kernel='sigmoid', gamma=2, C=10, probability=True),\n",
    "    #SVC(kernel='sigmoid', gamma=2, C=100, probability=True),\n",
    "#    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     GaussianProcessClassifier(2.0 * RBF(1.0)),\n",
    "    GaussianProcessClassifier(),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    #MLPClassifier(alpha=1e-05, max_iter=1000),\n",
    "    MLPClassifier(alpha=0.1, max_iter=1000),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    AdaBoostClassifier(n_estimators=10)]\n",
    "    #AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "# names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "#          \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "#          \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "# classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025, probability=True),\n",
    "#     SVC(gamma=2, C=1, probability=True),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "#     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     AdaBoostClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "\n",
    "\n",
    "NA_positive = pd.read_csv(Utils.get_ml_input_dir() + 'NA_positive_deposits.csv')\n",
    "NA_negative = pd.read_csv(Utils.get_ml_input_dir() + 'NA_negative_deposits.csv')\n",
    "\n",
    "NA_train_test_data = pd.concat([NA_positive, NA_negative])\n",
    "\n",
    "NA_labels = NA_train_test_data.iloc[:,-1]\n",
    "NA_data = NA_train_test_data.iloc[:,:-1]\n",
    "\n",
    "NA_data = preprocessing.scale(NA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(NA_data, NA_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "appended_data = pd.DataFrame()\n",
    "df = pd.DataFrame()\n",
    "# appended_data = pd.DataFrame(columns = ['fpr', 'tpr', 'thresh'])\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(train_data,train_labels)\n",
    "    pred_prob = clf.predict_proba(test_data)\n",
    "    fpr, tpr, thresh = roc_curve(test_labels, pred_prob[:,1], pos_label=1)\n",
    "    random_probs = [0 for i in range(len(test_labels))]\n",
    "    p_fpr, p_tpr, _ = roc_curve(test_labels, random_probs, pos_label=1)\n",
    "    auc_score = roc_auc_score(test_labels, pred_prob[:,1])\n",
    "    df['fpr']=pd.Series(fpr)\n",
    "    df['tpr']=pd.Series(tpr)\n",
    "    df['thresh']=pd.Series(thresh)\n",
    "    df['clf']=name\n",
    "    df['auc_score'] = auc_score\n",
    "    appended_data = appended_data.append(df, ignore_index=True)\n",
    "    \n",
    "# print(appended_data)\n",
    "appended_data.to_csv(r'case_AREPS/ml_output/NA_auc-roc.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
