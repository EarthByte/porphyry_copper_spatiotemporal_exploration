{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the parameters in use. Check the parameters carefully to ensure a sucussful run.\n",
      "{ 'agegrid_url': 'https://www.earthbyte.org/webdav/ftp/Data_Collections/Muller_etal_2016_AREPS/Muller_etal_2016_AREPS_Agegrids/Muller_etal_2016_AREPS_Agegrids_v1.17/Muller_etal_2016_AREPS_v1.17_netCDF/Muller_etal_2016_AREPS_v1.17_AgeGrid-{}.nc',\n",
      "  'anchor_plate_id': 0,\n",
      "  'case_name': 'case_AREPS',\n",
      "  'coastlines': '../data/Global_EarthByte_230-0Ma_GK07_AREPS_Coastlines.gpmlz',\n",
      "  'convergence_data_dir': './convergence_data/AREPS/',\n",
      "  'convergence_data_filename_ext': 'csv',\n",
      "  'convergence_data_filename_prefix': 'subStats',\n",
      "  'coreg_input_dir': './coreg_input/AREPS/',\n",
      "  'coreg_input_files': [ 'deposit_candidates.csv',\n",
      "                         'negative_deposits.csv',\n",
      "                         'positive_deposits.csv'],\n",
      "  'coreg_output_dir': 'coreg_output',\n",
      "  'feature_names': [ 'conv_rate',\n",
      "                     'dist_nearest_edge',\n",
      "                     'subduction_volume_km3y',\n",
      "                     'carbonate_sediment_thickness',\n",
      "                     'ocean_crust_carb_percent'],\n",
      "  'grid_files': [ [ '../data/AgeGrids/Muller_etal_2016_AREPS_v1.17_AgeGrid-{time:d}.nc',\n",
      "                    'seafloor_age'],\n",
      "                  [ '../data/carbonate_sed_thickness/decompacted_sediment_thickness_0.5_{time:d}.nc',\n",
      "                    'carbonate_sediment_thickness'],\n",
      "                  [ '../data/predicted_oceanic_sediment_thickness/sed_thick_0.2d_{time:d}.nc',\n",
      "                    'total_sediment_thick'],\n",
      "                  [ '../data/ocean_crust_CO2_grids/ocean_crust_carb_percent_{time:d}.nc',\n",
      "                    'ocean_crust_carb_percent']],\n",
      "  'machine_learning_engine': 'RFC',\n",
      "  'map_extent': [-85, -29, -55, 15],\n",
      "  'ml_input_dir': 'ml_input',\n",
      "  'ml_output_dir': 'ml_output',\n",
      "  'overwrite_existing_convergence_data': True,\n",
      "  'plate_tectonic_tools_path': '/workspace/spatio-temporal-exploration-master-JDR-SA-CZ/python/../PlateTectonicTools/ptt/',\n",
      "  'region_of_interest_polygon_file': '../data/polygon_north_america_julian.csv',\n",
      "  'regions': [5, 10, 30],\n",
      "  'rotation_files': ['../data/Global_EarthByte_230-0Ma_GK07_AREPS.rot'],\n",
      "  'static_polygons_file': '../data/Shapefiles/StaticPolygons/Global_EarthByte_GPlates_PresentDay_StaticPlatePolygons_2015_v1.shp',\n",
      "  'terranes': '',\n",
      "  'threshold_sampling_distance_degrees': 0.2,\n",
      "  'time': {'end': 230, 'start': 0, 'step': 1},\n",
      "  'topo_grid': '../data/topo15_3600x1800.nc',\n",
      "  'topology_files': [ '../data/Global_EarthByte_230-0Ma_GK07_AREPS_PlateBoundaries.gpml.gz',\n",
      "                      '../data/Global_EarthByte_230-0Ma_GK07_AREPS_Topology_BuildingBlocks.gpml.gz'],\n",
      "  'vector_files': ['{conv_dir}subStats_{time:.2f}.csv'],\n",
      "  'velocity_delta_time': 1}\n"
     ]
    }
   ],
   "source": [
    "from parameters import parameters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Utils\n",
    "#\n",
    "# LOOK HERE! \n",
    "# This cell must run first to setup the working environment\n",
    "#\n",
    "\n",
    "#load the config file\n",
    "Utils.load_config('config.json')\n",
    "Utils.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from parameters import parameters \n",
    "import Utils as Utils\n",
    "\n",
    "names = [\"GaussianProcess\", \"Neural_Network\", \"SVC_POLY\", \"SVC_RBF\", \"AdaBoost\", \"Random_Forest\",\n",
    "        \"Decision_Tree\",\"QDA\", \"Naive_Bayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    GaussianProcessClassifier(),\n",
    "    MLPClassifier(alpha=0.1, max_iter=10000),\n",
    "    SVC(kernel='poly', degree=3, gamma=2, C=10, probability=True),\n",
    "    SVC(kernel='rbf', gamma=2, C=10, probability=True),\n",
    "    AdaBoostClassifier(n_estimators=10),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    GaussianNB()]\n",
    "\n",
    "\n",
    "#NA_positive = pd.read_csv(Utils.get_ml_input_dir() + 'NA_positive_deposits.csv')\n",
    "#NA_negative = pd.read_csv(Utils.get_ml_input_dir() + 'NA_negative_deposits.csv')\n",
    "SA_positive = pd.read_csv(Utils.get_ml_input_dir() + 'SA_positive_deposits.csv')\n",
    "SA_negative = pd.read_csv(Utils.get_ml_input_dir() + 'SA_negative_deposits.csv')\n",
    "\n",
    "#NA_train_test_data = pd.concat([NA_positive, NA_negative])\n",
    "SA_train_test_data = pd.concat([SA_positive, SA_negative])\n",
    "\n",
    "#NA_labels = NA_train_test_data.iloc[:,-1]\n",
    "SA_labels = SA_train_test_data.iloc[:,-1]\n",
    "#NA_data = NA_train_test_data.iloc[:,:-1]\n",
    "SA_data = SA_train_test_data.iloc[:,:-1]\n",
    "\n",
    "#NA_data = preprocessing.scale(NA_data)\n",
    "SA_data = preprocessing.scale(SA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = ShuffleSplit(n_splits=10, test_size=.1, random_state=None)\n",
    "# #rs = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# NA_all_scores_cv = []\n",
    "# for name, clf in zip(names, classifiers):\n",
    "#     scores = cross_val_score(clf, NA_data, NA_labels, cv=rs)\n",
    "# #     score_list = scores.tolist()\n",
    "# #     NA_all_scores_cv.append(name)\n",
    "# #     NA_all_scores_cv.append(score_list)\n",
    "#     print(name)\n",
    "#     print(f\"Score Mean: {round(np.mean(scores), 2)}, Score Standard Deviation: {round(np.std(scores),4)}\")\n",
    "#     print(' ')\n",
    "\n",
    "# #np.savetxt('case_AREPS/ml_output/NA_cv_scores_ShuffleSplit_10_2.csv', NA_all_scores_cv, fmt='%s\\n', delimiter=\",\")\n",
    "\n",
    "# print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianProcess\n",
      "Score Mean: 0.95, Score Standard Deviation: 0.0437\n",
      " \n",
      "Neural_Network\n",
      "Score Mean: 0.96, Score Standard Deviation: 0.0516\n",
      " \n",
      "SVC_POLY\n",
      "Score Mean: 0.95, Score Standard Deviation: 0.0497\n",
      " \n",
      "SVC_RBF\n",
      "Score Mean: 0.98, Score Standard Deviation: 0.0241\n",
      " \n",
      "AdaBoost\n",
      "Score Mean: 0.96, Score Standard Deviation: 0.0337\n",
      " \n",
      "Random_Forest\n",
      "Score Mean: 0.96, Score Standard Deviation: 0.0529\n",
      " \n",
      "Decision_Tree\n",
      "Score Mean: 0.95, Score Standard Deviation: 0.0333\n",
      " \n",
      "QDA\n",
      "Score Mean: 0.97, Score Standard Deviation: 0.0674\n",
      " \n",
      "Naive_Bayes\n",
      "Score Mean: 0.86, Score Standard Deviation: 0.0529\n",
      " \n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "rs = ShuffleSplit(n_splits=10, test_size=.1, random_state=None)\n",
    "#rs = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "SA_all_scores_cv = []\n",
    "for name, clf in zip(names, classifiers):  \n",
    "    scores = cross_val_score(clf, SA_data, SA_labels, cv=rs)\n",
    "#    score_list = scores.tolist()\n",
    "#    SA_all_scores_cv.append(name)\n",
    "#    SA_all_scores_cv.append(score_list)\n",
    "    print(name)\n",
    "    print(f\"Score Mean: {round(np.mean(scores), 2)}, Score Standard Deviation: {round(np.std(scores),4)}\")\n",
    "    print(' ')\n",
    "        \n",
    "#np.savetxt('case_AREPS/ml_output/SA_cv_scores_ShuffleSplit_5.csv', SA_all_scores_cv, fmt='%s\\n', delimiter=\",\")\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from netCDF4 import Dataset\n",
    "import TopoMap\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import misc, ndimage\n",
    "\n",
    "#NA_positive = pd.read_csv(Utils.get_ml_input_dir() + 'NA_positive_deposits.csv')\n",
    "#NA_negative = pd.read_csv(Utils.get_ml_input_dir() + 'NA_negative_deposits.csv')\n",
    "SA_positive = pd.read_csv(Utils.get_ml_input_dir() + 'SA_positive_deposits.csv')\n",
    "SA_negative = pd.read_csv(Utils.get_ml_input_dir() + 'SA_negative_deposits.csv')\n",
    "\n",
    "#NA_train_test_data = pd.concat([NA_positive, NA_negative])\n",
    "SA_train_test_data = pd.concat([SA_positive, SA_negative])\n",
    "\n",
    "#NA_labels = NA_train_test_data.iloc[:,-1]\n",
    "SA_labels = SA_train_test_data.iloc[:,-1]\n",
    "#NA_data = NA_train_test_data.iloc[:,:-1]\n",
    "SA_data = SA_train_test_data.iloc[:,:-1]\n",
    "\n",
    "#NA_data = preprocessing.scale(NA_data)\n",
    "SA_data = preprocessing.scale(SA_data)\n",
    "\n",
    "#NA_train_data, NA_test_data, NA_train_labels, NA_test_labels = train_test_split(NA_data, NA_labels, test_size=0.4, random_state=None)\n",
    "\n",
    "df = pd.DataFrame(columns=['lat', 'lon', 'recon_lon', 'recon_lat', 'age', 'prob', 'method']) \n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(name)\n",
    "    \n",
    "    clf.fit(SA_data, SA_labels)\n",
    "    \n",
    "    SA_candidates= pd.read_csv(Utils.get_ml_input_dir() + 'SA_deposit_candidates.csv')\n",
    "    \n",
    "    SA_all_candidates_data = pd.read_csv('case_AREPS/ml_input/SA_candidates_all_columns.csv')\n",
    "    SA_all_positive_data = pd.read_csv('case_AREPS/ml_input/SA_positive_all_columns.csv')\n",
    "    SA_all_negative_data = pd.read_csv('case_AREPS/ml_input/SA_negative_all_columns.csv')\n",
    "    #NA_all_candidates_data = pd.read_csv('case_AREPS/coreg_output_clean/NA/deposit_candidates.csv', index_col =\"age\" )\n",
    "\n",
    "    feature_names = Utils.get_parameter('feature_names')\n",
    "\n",
    "    SA_positive_idx = SA_all_positive_data[feature_names].dropna().index\n",
    "    SA_negative_idx = SA_all_negative_data[feature_names].dropna().index\n",
    "    SA_candidates_idx = SA_all_candidates_data[feature_names].dropna().index\n",
    "    \n",
    "    SA_test_data = preprocessing.scale(SA_candidates)\n",
    "    SA_mesh_prob=clf.predict_proba(SA_test_data)\n",
    "\n",
    "    SA_candidates_lat_lon = SA_all_candidates_data.iloc[SA_candidates_idx][['lat', 'lon', 'recon_lon', 'recon_lat', 'age']]\n",
    "    print(SA_candidates_lat_lon.shape) ###1  \n",
    "    SA_candidates_lat_lon['prob'] = SA_mesh_prob[:,1]\n",
    "    print(SA_candidates_lat_lon.shape)\n",
    "    SA_candidates_lat_lon['method']=name\n",
    "    print(SA_candidates_lat_lon.shape)\n",
    "    df = pd.concat([df, SA_candidates_lat_lon]) \n",
    "    SA_candidates_lat_lon.to_csv('case_AREPS/ml_output/SA/SA_candidates_all_prob_'+name+'.csv', index=True)\n",
    "    \n",
    "    \n",
    "# #     print(NA_candidates_lat_lon.shape) ###3\n",
    "    \n",
    "# #     NA_candidates_lat_lon = NA_candidates_lat_lon[NA_candidates_lat_lon.age > met_age_min]\n",
    "# #     NA_candidates_lat_lon = NA_candidates_lat_lon[NA_candidates_lat_lon.age < met_age_max]\n",
    "# # #    print(NA_candidates_lat_lon.shape) ###2 \n",
    "    \n",
    "\n",
    "# # #    print(NA_candidates_lat_lon.shape) ###2 \n",
    "    \n",
    "#     NA_candidates_lat_lon_group = NA_candidates_lat_lon.groupby(['recon_lon','recon_lat'])['prob'].mean()\n",
    "#     print(NA_candidates_lat_lon_group.shape) ###4\n",
    "#     NA_candidates_lat_lon_group.to_csv('case_AREPS/ml_output/NA/NA_candidates_lat_lon_all_ages_'+name+'.csv', index=True)\n",
    "\n",
    "\n",
    "#    SA_candidates_lat_lon_group = SA_candidates_lat_lon.groupby(['lon','lat'])['prob'].mean()\n",
    "#    print(NA_candidates_lat_lon_group.shape) ###4\n",
    "#    SA_candidates_lat_lon_group.to_csv('case_AREPS/ml_output/SA/SA_candidates_all_mean_prob_'+name+'.csv', index=True)\n",
    "    \n",
    "        \n",
    "# #     NA_candidates_lat_lon = NA_candidates_lat_lon.reset_index()\n",
    "# #     print(NA_candidates_lat_lon.shape) ###5\n",
    "    \n",
    "# # #    NA_candidates_lat_lon.to_csv('case_AREPS/ml_output/NA/NA_candidates_mean_all_ages_'+name+'.csv', index=True)\n",
    "#     NA_candidates_lat_lon.to_csv('case_AREPS/ml_output/NA/NA_candidates_recon_lat_lon_age_'+name+'.csv', index=True)\n",
    "\n",
    "\n",
    "#df.to_csv('case_AREPS/ml_output/SA/SA_candidates_all_prob_all_classifers.csv', index=True) #Export all data, better go down and export this + positives and Negatives\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_prob_adaboost = pd.read_csv('case_AREPS/ml_output/NA/NA_candidates_all_prob_AdaBoost-10.csv')\n",
    "# all_prob_GaussianProcess = pd.read_csv('case_AREPS/ml_output/NA/NA_candidates_all_prob_GaussianProcess-1.csv')\n",
    "# all_prob_NN = pd.read_csv('case_AREPS/ml_output/NA/NA_candidates_all_prob_NN-0_1.csv')\n",
    "# all_prob_RandomForest = pd.read_csv('case_AREPS/ml_output/NA/NA_candidates_all_prob_RandomForest_2.csv')\n",
    "# all_prob_svc_poly = pd.read_csv('case_AREPS/ml_output/NA/NA_candidates_all_prob_SVC-POLY-C_10.csv')\n",
    "# all_prob_svc_rbf = pd.read_csv('case_AREPS/ml_output/NA/NA_candidates_all_prob_SVC-RBF-C_10.csv')\n",
    "\n",
    "# files = [all_prob_adaboost, all_prob_GaussianProcess, all_prob_NN, all_prob_RandomForest, all_prob_svc_poly, all_prob_svc_rbf]\n",
    "\n",
    "# for file in files:\n",
    "#     print(file)\n",
    "#     NA_candidates_all_ages_mean = file.groupby(['lon','lat'])['prob'].mean()\n",
    "#     print(NA_candidates_all_ages_mean.shape)\n",
    "#     NA_candidates_all_ages_mean.to_csv('case_AREPS/ml_output/NA/NA_candidates_all_mean_prob_'+name+'.csv', index=True)\n",
    "#     print('Ok!')\n",
    "\n",
    "# print('finish')\n",
    "\n",
    "\n",
    "SA_all_positive_data = pd.read_csv('case_AREPS/ml_input/SA_positive_all_columns.csv')\n",
    "SA_all_negative_data = pd.read_csv('case_AREPS/ml_input/SA_negative_all_columns.csv')\n",
    "#NA_all_candidates_data = pd.read_csv('case_AREPS/coreg_output_clean/NA/deposit_candidates.csv')\n",
    "\n",
    "\n",
    "feature_names = Utils.get_parameter('feature_names')\n",
    "\n",
    "SA_positive_idx = SA_all_positive_data[feature_names].dropna().index\n",
    "SA_negative_idx = SA_all_negative_data[feature_names].dropna().index\n",
    "SA_candidates_idx = SA_all_candidates_data[feature_names].dropna().index\n",
    "\n",
    "\n",
    "SA_positives_lat_lon = SA_all_positive_data.iloc[SA_positive_idx][['lat', 'lon', 'recon_lon', 'recon_lat', 'age']]\n",
    "SA_negatives_lat_lon = SA_all_negative_data.iloc[SA_negative_idx][['lat', 'lon', 'recon_lon', 'recon_lat', 'age']]    \n",
    "\n",
    "SA_positives_lat_lon['prob'] = 10\n",
    "SA_negatives_lat_lon['prob'] = 20\n",
    "SA_positives_lat_lon['method']='positives'\n",
    "SA_negatives_lat_lon['method']='negatives'\n",
    "\n",
    "df_2 = pd.DataFrame(columns=['lat', 'lon', 'recon_lon', 'recon_lat', 'age', 'prob', 'method']) \n",
    "\n",
    "df_2 = pd.concat([SA_positives_lat_lon, SA_negatives_lat_lon, df])\n",
    "\n",
    "####df_2.to_csv('case_AREPS/ml_output/SA/SA_candidates_pos_neg_all_prob_.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
